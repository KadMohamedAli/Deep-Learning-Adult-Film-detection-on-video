{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d822fe-3fa8-4717-b5d4-1de8fde90526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders as sf\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b0d738e-7b43-4289-bca3-73ae3aac9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir=Path(\"A:\\AI DB\\LSPD\\Videos\\photos_from_videos\")\n",
    "\n",
    "output_dir=Path(\"A:\\AI DB\\LSPD\\Videos\\photos_from_videos_pret\")\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6faf7427-3c77-44d6-bdcb-293ae9058984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 973644 files [57:40, 281.36 files/s]\n"
     ]
    }
   ],
   "source": [
    "# split dataset 90% training 5% validation 5% testing\n",
    "sf.ratio(dataset_dir,output_dir,seed=1337,ratio=(.9,.05,.05),group_prefix=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "724d2384-32ff-4851-b591-46225e706788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import splitfolders as sf\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import PIL\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, concatenate, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import seaborn as sns\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB4\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "class CustomLearningRateScheduler(LearningRateScheduler):\n",
    "    def __init__(self, schedule, patience=0, factor=0.1, min_lr=0, filepath=None, save_best_only=False, **kwargs):\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.filepath = filepath\n",
    "        self.save_best_only = save_best_only\n",
    "        super(CustomLearningRateScheduler, self).__init__(schedule, **kwargs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Your custom logic here\n",
    "        # You can access self.patience, self.factor, etc. here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "de78d78c-a77e-4911-b296-4b342e375917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    try:\n",
    "        # Perform preprocessing operations on the image\n",
    "        # Example: resize, normalize, etc.\n",
    "        # Make sure to return the preprocessed image\n",
    "        preprocessed_image = preprocess_input(image)\n",
    "        return preprocessed_image\n",
    "    except Exception as e:\n",
    "        # Handle the exception (e.g., log error, skip image)\n",
    "        # For now, just return None to indicate that the image should be skipped\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e52f2f3d-7dda-4aee-9cfa-e18bc16af48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datagen():\n",
    "    return ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        preprocessing_function=preprocess_input\n",
    "    )\n",
    "\n",
    "def create_generators(train_dir, validation_dir, test_dir, img_height, img_width, batch_size):\n",
    "    train_datagen = create_datagen()\n",
    "    validation_datagen = ImageDataGenerator(rescale=1./255,\n",
    "        preprocessing_function=preprocess_input)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255,\n",
    "        preprocessing_function=preprocess_input\n",
    "        )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "    \n",
    "    return train_generator, validation_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2d6f97b1-2fe9-4c06-8788-1a8839e58b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    base_model = EfficientNetB4(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "    # Freeze the base model\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Add custom layers on top of the base model\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)  # Adding Dropout for regularization\n",
    "    predictions = Dense(1, activation='sigmoid', dtype='float32')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # Count trainable and non-trainable parameters\n",
    "    trainable_count = np.sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "    non_trainable_count = np.sum([tf.keras.backend.count_params(w) for w in model.non_trainable_weights])\n",
    "    print(\"Trainable parameters:\", trainable_count)\n",
    "    print(\"Non-trainable parameters:\", non_trainable_count)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9760c678-8ab4-4e2e-b273-f363c38652f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_schedule(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.5\n",
    "\n",
    "def create_callbacks():\n",
    "    lr_scheduler = CustomLearningRateScheduler(\n",
    "        schedule=custom_schedule,  # Use your custom schedule function here\n",
    "        patience=2, \n",
    "        factor=0.5, \n",
    "        min_lr=1e-6, \n",
    "        filepath=r'A:\\AI DB\\LSPD\\models\\efficienttrainedonimagefromvideos.keras', \n",
    "        save_best_only=True\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_accuracy', \n",
    "        patience=3, \n",
    "        verbose=1, \n",
    "        mode='max'\n",
    "    )\n",
    "    return [lr_scheduler, early_stopping]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b59f850c-884d-45f5-a2ed-088a70ee19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dir, validation_dir, test_dir, img_height, img_width, batch_size):\n",
    "    # Assuming that the dataset is organized as per Keras ImageDataGenerator requirements\n",
    "    train_generator, validation_generator, test_generator = create_generators(\n",
    "        train_dir, validation_dir, test_dir, img_height, img_width, batch_size)\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(train_generator.classes),y= train_generator.classes)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    model = build_model()\n",
    "    callbacks = create_callbacks()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=4,\n",
    "        validation_data=validation_generator,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Unfreeze the last block of layers after a certain number of epochs (e.g., 5)\n",
    "    for layer in model.layers[-20:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Compile the model after unfreezing\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Continue training with the unfrozen layers\n",
    "    history_unfrozen = model.fit(\n",
    "        train_generator,\n",
    "        epochs=6,  # Continue training for additional epochs\n",
    "        initial_epoch=3,  # Start from epoch 5 after unfreezing\n",
    "        validation_data=validation_generator,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # Concatenate the histories\n",
    "    history.history['loss'].extend(history_unfrozen.history['loss'])\n",
    "    history.history['accuracy'].extend(history_unfrozen.history['accuracy'])\n",
    "    history.history['val_loss'].extend(history_unfrozen.history['val_loss'])\n",
    "    history.history['val_accuracy'].extend(history_unfrozen.history['val_accuracy'])\n",
    "        \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ccadea28-2b2d-42de-86a8-68ec130c7833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_generator, model,threshold):\n",
    "    # Predict on the test data\n",
    "    ypred = model.predict(test_generator)\n",
    "    y_pred_classes = (ypred > threshold).astype(int)\n",
    "\n",
    "    # Get actual classes\n",
    "    y_true_classes = test_generator.classes\n",
    "\n",
    "    # Create a confusion matrix\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "    # Normalize the confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Generate a classification report\n",
    "    cr = classification_report(y_true_classes, y_pred_classes, digits=4)\n",
    "    print(\"Classification Report:\\n\", cr)\n",
    "\n",
    "    # Plot the confusion matrix with annotations positioned at the bottom\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, cmap='YlGnBu', fmt='d', xticklabels=['Normal', 'Porn'], yticklabels=['Normal', 'Porn'], annot_kws={'size': 16})\n",
    "\n",
    "    # Add custom annotations at the bottom of each cell\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j + 0.5, i + 0.2, '{:.2f}%'.format(cm_normalized[i, j] * 100), ha='center', va='bottom', color='black', fontsize=14)\n",
    "\n",
    "    plt.ylabel('Actual', fontsize=12)\n",
    "    plt.xlabel('Predicted', fontsize=12)\n",
    "    plt.title('Confusion Matrix', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate accuracy score\n",
    "    accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "    print(\"Accuracy Score:\", format(accuracy, '.4f'))\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_true_classes, y_pred_classes)\n",
    "    print(\"F1 Score:\", format(f1, '.4f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2e5b73a5-3693-436f-adf4-686305724116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_threshold(test_generator, model):\n",
    "    ypred = model.predict(test_generator)\n",
    "    thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred_classes = (ypred > threshold).astype(int)\n",
    "        f1 = f1_score(test_generator.classes, y_pred_classes)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print(f\"Best threshold: {best_threshold} with F1 score: {best_f1}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "34629a45-ab08-4e47-8a28-a6d3fdde6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'A:\\AI DB\\LSPD\\Videos\\photos_from_videos_pret\\train'\n",
    "validation_dir = r'A:\\AI DB\\LSPD\\Videos\\photos_from_videos_pret\\val'\n",
    "test_dir = r'A:\\AI DB\\LSPD\\Videos\\photos_from_videos_pret\\test'\n",
    "\n",
    "save_path=r'A:\\AI DB\\LSPD\\models\\inceptionTrainedOnVideos.keras'\n",
    "# Define image dimensions and batch size\n",
    "img_height, img_width = 380, 380\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9520c361-02e3-4f69-bd2f-6a97a9b3a979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts : \n",
      "Found 430877 images belonging to 2 classes.\n",
      "Found 23937 images belonging to 2 classes.\n",
      "Found 23939 images belonging to 2 classes.\n",
      "Trainable parameters: 1837057\n",
      "Non-trainable parameters: 17673823.0\n",
      "Epoch 1/4\n",
      "6733/6733 [==============================] - 11265s 2s/step - loss: 0.6897 - accuracy: 0.5554 - val_loss: 0.6758 - val_accuracy: 0.6236\n",
      "Epoch 2/4\n",
      "6733/6733 [==============================] - 11132s 2s/step - loss: 0.6850 - accuracy: 0.5816 - val_loss: 0.6813 - val_accuracy: 0.5934\n",
      "Epoch 3/4\n",
      "5119/6733 [=====================>........] - ETA: 42:43 - loss: 0.6836 - accuracy: 0.5882"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining starts : \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Train the model using cross-validation\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m histories \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished .\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the best model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[83], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_dir, validation_dir, test_dir, img_height, img_width, batch_size)\u001b[0m\n\u001b[0;32m     11\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m create_callbacks()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Unfreeze the last block of layers after a certain number of epochs (e.g., 5)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m:]:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\amchibrk\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\amchibrk\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\amchibrk\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\amchibrk\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\amchibrk\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\amchibrk\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\amchibrk\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\amchibrk\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\amchibrk\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training starts : \")\n",
    "# Train the model using cross-validation\n",
    "histories = train(train_dir, validation_dir, test_dir, img_height, img_width, batch_size)\n",
    "print(\"Training finished .\")\n",
    "# Load the best model\n",
    "model = tf.keras.models.load_model(save_path)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "print(\"Evaluation : \")\n",
    "evaluate_model(test_generator, model,0.5)\n",
    "print(\"Calculating best threshold : \")\n",
    "# Tune threshold\n",
    "best_threshold = tune_threshold(test_generator, model)\n",
    "print(\"Best threshold : \",str(best_threshold))\n",
    "# Re-evaluate using the best threshold\n",
    "print(\"Evaluation with new threshold : \")\n",
    "evaluate_model(test_generator, model,best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8df778-d7f4-4db5-9312-bb6eee6ac15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
